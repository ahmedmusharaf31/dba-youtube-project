{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547bfe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (0.19.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\ahmed\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for re\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: praw in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "# reddit_rF1_pipeline.py\n",
    "\n",
    "%pip install textblob praw pandas re datetime\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reddit API setup\n",
    "# NOTE: replace these with our own credentials\n",
    "CLIENT_ID = \"your_client_id\"\n",
    "CLIENT_SECRET = \"your_client_secret\"\n",
    "USER_AGENT = \"ds464-rf1-project by u/Hefty_Affect350 v1.0\"\n",
    "reddit = praw.Reddit(\n",
    "client_id=CLIENT_ID,\n",
    "client_secret=CLIENT_SECRET,\n",
    "user_agent=USER_AGENT\n",
    ")\n",
    "subreddit = reddit.subreddit(\"formula1\") # some F1 content is also here\n",
    "# if we strictly want r/F1 and have access, we can switch the name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f46aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2 23]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# 2. Helper functions for cleaning text\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning for reddit comments and posts.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # remove markdown links [text](url)\n",
    "    text = re.sub(r\"\\[.*?\\]\\(.*?\\)\", \" \", text)\n",
    "    # remove special characters and digits (keep basic punctuation)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s']\", \" \", text)\n",
    "    # collapse extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"Return simple sentiment score using TextBlob.\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    blob = TextBlob(text)\n",
    "    return float(blob.sentiment.polarity) # -1 (negative) to 1\n",
    "    (positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d55e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract posts and comments\n",
    "posts_data = []\n",
    "comments_data = []\n",
    "\n",
    "# we take top posts from the last year as a starting point\n",
    "for submission in subreddit.top(time_filter=\"year\", limit=200):\n",
    "    # skip removed or deleted\n",
    "    if submission.selftext in [\"[removed]\", \"[deleted]\"]:\n",
    "        continue\n",
    "\n",
    "    post_record = {\n",
    "        \"post_id\": submission.id,\n",
    "        \"title\": submission.title,\n",
    "        \"body\": submission.selftext,\n",
    "        \"score\": submission.score,\n",
    "        \"num_comments\": submission.num_comments,\n",
    "        \"created_utc\": datetime.fromtimestamp(submission.created_utc),\n",
    "        \"author\": str(submission.author),\n",
    "        \"flair\": str(submission.link_flair_text)\n",
    "    }\n",
    "    posts_data.append(post_record)\n",
    "\n",
    "    # load comments for this post\n",
    "    submission.comments.replace_more(limit=0)\n",
    "for comment in submission.comments.list():\n",
    "        if isinstance(comment.body, str) and comment.body not in [\"[removed]\", \"[deleted]\"]:\n",
    "            comments_data.append({\n",
    "                \"comment_id\": comment.id,\n",
    "                \"post_id\": submission.id,\n",
    "                \"author\": str(comment.author),\n",
    "                \"body\": comment.body,\n",
    "                \"score\": comment.score,\n",
    "                \"created_utc\":\n",
    "                datetime.fromtimestamp(comment.created_utc),\n",
    "                \"parent_id\": comment.parent_id\n",
    "            })\n",
    "            \n",
    "# create dataframes\n",
    "posts_df = pd.DataFrame(posts_data)\n",
    "comments_df = pd.DataFrame(comments_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocess text and add sentiment\n",
    "# clean titles and bodies\n",
    "\n",
    "posts_df[\"clean_title\"] = posts_df[\"title\"].apply(clean_text)\n",
    "posts_df[\"clean_body\"] = posts_df[\"body\"].apply(clean_text)\n",
    "\n",
    "# sentiment for post body (average mood of original post)\n",
    "posts_df[\"body_sentiment\"] = posts_df[\"clean_body\"].apply(get_sentiment)\n",
    "\n",
    "# clean comments and add sentiment\n",
    "comments_df[\"clean_body\"] = comments_df[\"body\"].apply(clean_text)\n",
    "comments_df[\"sentiment\"] =\n",
    "comments_df[\"clean_body\"].apply(get_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30aba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Derive simple engagement features\n",
    "# engagement score for posts (very simple version)\n",
    "posts_df[\"engagement_score\"] = posts_df[\"num_comments\"] /(posts_df[\"score\"] + 1)\n",
    "\n",
    "# basic time attributes\n",
    "posts_df[\"date\"] = posts_df[\"created_utc\"].dt.date\n",
    "posts_df[\"year\"] = posts_df[\"created_utc\"].dt.year\n",
    "posts_df[\"month\"] = posts_df[\"created_utc\"].dt.month\n",
    "comments_df[\"date\"] = comments_df[\"created_utc\"].dt.date\n",
    "comments_df[\"year\"] = comments_df[\"created_utc\"].dt.year\n",
    "comments_df[\"month\"] = comments_df[\"created_utc\"].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62128d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save cleaned data for warehouse loading\n",
    "posts_df.to_csv(\"rf1_posts_clean.csv\", index=False)\n",
    "comments_df.to_csv(\"rf1_comments_clean.csv\", index=False)\n",
    "print(\"Saved rf1_posts_clean.csv and rf1_comments_clean.csv\")\n",
    "\n",
    "print(\"Reddit r/F1 data extraction and preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f95c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
